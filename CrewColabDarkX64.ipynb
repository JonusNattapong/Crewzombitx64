{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåô CrewColabDarkX64 - Web Scraping with Local AI\n",
    "\n",
    "**Dark Mode Edition**\n",
    "\n",
    "This notebook provides:\n",
    "1. üîó Google Drive integration\n",
    "2. ü§ñ Local AI model usage\n",
    "3. üåê Web scraping functionality\n",
    "4. üíæ Automated content saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ASCII Colors for Dark Mode\n",
    "class Colors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    DIM = '\\033[2m'\n",
    "    PURPLE = '\\033[35m'\n",
    "    CYAN = '\\033[36m'\n",
    "\n",
    "# Print ASCII Art\n",
    "ascii_art = f\"\"\"{Colors.PURPLE}\n",
    "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó\n",
    "‚ïö‚ïê‚ïê‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë\n",
    "  ‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë    ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë\n",
    " ‚ñà‚ñà‚ñà‚ïî‚ïù  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë\n",
    "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù     ‚ñà‚ñà‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù      ‚ïö‚ïê‚ïù\n",
    "{Colors.ENDC}\"\"\"\n",
    "\n",
    "print(ascii_art)\n",
    "\n",
    "# Print Disclaimer\n",
    "print(f\"\\n{Colors.WARNING}‚ö†Ô∏è DISCLAIMER AND WARNING / ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö ‚ö†Ô∏è{Colors.ENDC}\")\n",
    "print(f\"{Colors.WARNING}=\" * 80)\n",
    "print(f\"{Colors.BOLD}üîí [EN] IMPORTANT DISCLAIMER:{Colors.ENDC}\")\n",
    "print(\"This tool is for educational purposes only.\")\n",
    "print(\"Users are fully responsible for their own actions.\")\n",
    "print(\"The developers assume no liability and are not responsible for any misuse or damage.\")\n",
    "print(\"\\nüö´ By using this tool, you agree to:\")\n",
    "print(\"1. Use it legally and ethically\")\n",
    "print(\"2. Respect website terms of service\")\n",
    "print(\"3. Not use for malicious purposes\")\n",
    "print(\"4. Take full responsibility for your actions\")\n",
    "\n",
    "print(f\"\\n{Colors.BOLD}üîí [TH] ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:{Colors.ENDC}\")\n",
    "print(\"‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\")\n",
    "print(\"‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á\")\n",
    "print(\"‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ô‡∏µ‡πâ\")\n",
    "print(\"\\n‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\")\n",
    "print(\"1. ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°\")\n",
    "print(\"2. ‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå\")\n",
    "print(\"3. ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢\")\n",
    "print(\"4. ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\")\n",
    "print(f\"\\n{Colors.FAIL}‚ùó ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‚ùó{Colors.ENDC}\")\n",
    "print(f\"{Colors.WARNING}=\" * 80 + f\"{Colors.ENDC}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install transformers beautifulsoup4 requests\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/ZombitX64_Output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚úÖ Output directory created: {output_dir}{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqGeneration\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚úÖ Libraries imported successfully{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Load Local AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"  # You can change this to other models\n",
    "\n",
    "print(f\"{Colors.OKBLUE}ü§ñ Loading model: {model_name}{Colors.ENDC}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚ú® Model loaded on {device}{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def check_robots_txt(url):\n",
    "    \"\"\"Check if crawling is allowed by robots.txt\"\"\"\n",
    "    try:\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        if not parsed_url.netloc:\n",
    "            parsed_url = urllib.parse.urlparse(\"http://\" + url)\n",
    "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "        \n",
    "        print(f\"{Colors.DIM}üîç Checking: {robots_url}{Colors.ENDC}\")\n",
    "        response = requests.get(robots_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return \"Disallow: /\" not in response.text\n",
    "    except Exception as e:\n",
    "        print(f\"{Colors.WARNING}‚ö†Ô∏è Error checking robots.txt: {str(e)}{Colors.ENDC}\")\n",
    "        return True\n",
    "\n",
    "def scrape_content(url):\n",
    "    \"\"\"Main scraping function\"\"\"\n",
    "    print(f\"\\n{Colors.HEADER}üîç Starting to scrape: {url}{Colors.ENDC}\")\n",
    "    \n",
    "    if not check_robots_txt(url):\n",
    "        print(f\"{Colors.FAIL}‚ùå Crawling disallowed by robots.txt{Colors.ENDC}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"{Colors.OKBLUE}üì° Fetching content...{Colors.ENDC}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('h1').text.strip() if soup.find('h1') else \"No Title Found\"\n",
    "        \n",
    "        content = []\n",
    "        article = soup.find('article') or soup.find(class_='entry-content')\n",
    "        \n",
    "        if article:\n",
    "            for elem in article.find_all(['p', 'h2', 'h3', 'ul', 'ol']):\n",
    "                content.append(elem.get_text(strip=True))\n",
    "        \n",
    "        result = {\n",
    "            \"title\": title,\n",
    "            \"text\": \"\\n\\n\".join(content),\n",
    "            \"metadata\": {\n",
    "                \"url\": url,\n",
    "                \"crawled_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"{Colors.OKGREEN}‚ú® Successfully scraped content ({len(result['text'])} characters){Colors.ENDC}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{Colors.FAIL}‚ùå Error scraping content: {str(e)}{Colors.ENDC}\")\n",
    "        return None\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚úÖ Functions defined successfully{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Text Summarization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def summarize_text(text, max_length=1024):\n",
    "    \"\"\"Generate content summary using local model\"\"\"\n",
    "    try:\n",
    "        print(f\"{Colors.OKBLUE}üìä Processing text...{Colors.ENDC}\")\n",
    "        inputs = tokenizer(text, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        print(f\"{Colors.DIM}‚è≥ Generating summary...{Colors.ENDC}\")\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        print(f\"{Colors.OKGREEN}‚ú® Summary generated successfully!{Colors.ENDC}\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{Colors.FAIL}‚ùå Error generating summary: {str(e)}{Colors.ENDC}\")\n",
    "        return \"Failed to generate summary.\"\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚úÖ Summarization function ready{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Results Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(content, summary):\n",
    "    \"\"\"Save results to Google Drive\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save JSON\n",
    "    print(f\"\\n{Colors.OKBLUE}üíæ Saving JSON...{Colors.ENDC}\")\n",
    "    json_path = f\"{output_dir}/scraped_{timestamp}.json\"\n",
    "    content_with_summary = {\n",
    "        **content,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_with_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save Markdown\n",
    "    print(f\"{Colors.OKBLUE}üìù Saving Markdown...{Colors.ENDC}\")\n",
    "    md_path = f\"{output_dir}/scraped_{timestamp}.md\"\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {content['title']}\\n\\n\")\n",
    "        f.write(f\"URL: {content['metadata']['url']}\\n\\n\")\n",
    "        f.write(f\"Crawled at: {content['metadata']['crawled_at']}\\n\\n\")\n",
    "        f.write(f\"## Summary\\n\\n{summary}\\n\\n\")\n",
    "        f.write(content['text'])\n",
    "    \n",
    "    print(f\"\\n{Colors.OKGREEN}‚ú® Results saved successfully:{Colors.ENDC}\")\n",
    "    print(f\"{Colors.DIM}    üìÑ JSON: {json_path}\")\n",
    "    print(f\"    üìù Markdown: {md_path}{Colors.ENDC}\")\n",
    "\n",
    "print(f\"{Colors.OKGREEN}‚úÖ Save function ready{Colors.ENDC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f\"{Colors.CYAN}üåô Dark Mode Web Scraping Tool{Colors.ENDC}\")\n",
    "\n",
    "# Input URL\n",
    "url = input(f\"{Colors.OKBLUE}üìå Enter URL (or press Enter for default): {Colors.ENDC}\") or \"https://example.com\"\n",
    "print(f\"\\n{Colors.CYAN}üîó URL: {url}{Colors.ENDC}\")\n",
    "\n",
    "# Scrape content\n",
    "content = scrape_content(url)\n",
    "\n",
    "if content:\n",
    "    # Generate summary\n",
    "    summary = summarize_text(content['text'])\n",
    "    \n",
    "    # Save results\n",
    "    save_results(content, summary)\n",
    "    \n",
    "    print(f\"\\n{Colors.OKGREEN}üéâ Operation completed successfully!{Colors.ENDC}\")\n",
    "else:\n",
    "    print(f\"\\n{Colors.FAIL}‚ùå Failed to crawl content{Colors.ENDC}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
