{
  "title": "Top AI/LLM learning resource in 2025",
  "text": "The Blog is organized into three main segments:\n\n1. LLM Fundamentals (optional) ‚Äì Covers essential topics such as mathematics, Python, and neural networks.\n2. The LLM Scientist ‚Äì Concentrates on creating the best-performing LLMs using state-of-the-art techniques.\n3. The LLM Engineer ‚Äì Focuses on building applications based on LLMs and deploying them.\n\n### üìù Notebooks\n\nBelow is a collection of notebooks and articles dedicated to LLMs.\n\n### Tools\n\n### Fine-tuning\n\n### Quantization\n\n### Other\n\n### LLM Fundamentals\n\nThis section provides core knowledge about mathematics, Python, and neural networks. While you may not begin here if you already have the basics, feel free to refer back as needed.\n\n### 1. Mathematics for Machine Learning\n\nBefore diving deep into machine learning, it is essential to master the fundamental mathematical concepts that underpin these algorithms:\n\n* Linear Algebra: Crucial for many algorithms, particularly in deep learning. Topics include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.\n* Calculus: Needed to optimize continuous functions. Learn about derivatives, integrals, limits, series, multivariable calculus, and gradient concepts.\n* Probability and Statistics: Key for understanding model behavior and data prediction. Essential topics include probability theory, random variables, distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.\n\nResources:\n\n* [3Blue1Brown ‚Äì The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n* [StatQuest with Josh Starmer ‚Äì Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9)\n* [AP Statistics Intuition by Ms Aerin](https://automata88.medium.com/list/cacc224d5e7d)\n* [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html)\n* [Khan Academy ‚Äì Linear Algebra](https://www.khanacademy.org/math/linear-algebra)\n* [Khan Academy ‚Äì Calculus](https://www.khanacademy.org/math/calculus-1)\n* [Khan Academy ‚Äì Probability and Statistics](https://www.khanacademy.org/math/statistics-probability)\n\n### 2. Python for Machine Learning\n\nPython is a flexible and powerful language, especially suited for machine learning because of its clear syntax and extensive ecosystem.\n\n* Python Basics: Understand basic syntax, data types, error handling, and object-oriented programming.\n* Data Science Libraries: Gain experience with NumPy for numerical operations; Pandas for data manipulation; and Matplotlib/Seaborn for visualizations.\n* Data Preprocessing: Learn techniques such as feature scaling, normalization, handling missing values, outlier detection, encoding categorical data, and data splitting.\n* Machine Learning Libraries: Familiarize yourself with Scikit-learn, which offers numerous supervised and unsupervised algorithms. Understand implementations of linear regression, logistic regression, decision trees, random forests, k-nearest neighbors, K-means clustering, and dimensionality reduction methods like PCA and t-SNE.\n\nResources:\n\n* [Real Python](https://realpython.com/)\n* [freeCodeCamp ‚Äì Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw)\n* [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n* [freeCodeCamp ‚Äì Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg)\n* [Udacity ‚Äì Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)\n\n### 3. Neural Networks\n\nNeural networks form the backbone of many modern deep learning models. It‚Äôs important to understand how they work and are built:\n\n* Fundamentals: Know the basic structure including layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.).\n* Training and Optimization: Get to know backpropagation, common loss functions (MSE, Cross-Entropy), and optimization algorithms (Gradient Descent, SGD, RMSprop, Adam).\n* Overfitting: Understand what overfitting means and study regularization techniques such as dropout, L1/L2 regularization, early stopping, and data augmentation.\n* Implementing a Multilayer Perceptron (MLP): Build an MLP (a fully connected network) using frameworks like PyTorch.\n\nResources:\n\n* [3Blue1Brown ‚Äì But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk)\n* [freeCodeCamp ‚Äì Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c)\n* [Fast.ai ‚Äì Practical Deep Learning](https://course.fast.ai/)\n* [Patrick Loeber ‚Äì PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4)\n\n### 4. Natural Language Processing (NLP)\n\nNLP is an exciting field that connects human language with machine comprehension. It ranges from basic text processing to capturing intricate linguistic nuances.\n\n* Text Preprocessing: Understand tokenization (dividing text into words or sentences), stemming (reducing words to their roots), lemmatization (context-aware reduction), and stop word removal.\n* Feature Extraction Techniques: Learn how to transform textual data for machine learning algorithms using techniques like Bag-of-Words (BoW), TF-IDF, and n-grams.\n* Word Embeddings: Study methods such as Word2Vec, GloVe, and FastText which allow words with similar meanings to have similar vector representations.\n* Recurrent Neural Networks (RNNs): Learn how RNNs are designed for sequential data and explore variants like LSTMs and GRUs, which capture long-term dependencies.\n\nResources:\n\n* [Lena Voita ‚Äì Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html)\n* [RealPython ‚Äì NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/)\n* [Kaggle ‚Äì NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing)\n* [Jay Alammar ‚Äì The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/)\n* [Jake Tae ‚Äì PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/)\n* [colah‚Äôs blog ‚Äì Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n\n### The LLM Scientist\n\nThis section is designed to help you learn how to build the most effective LLMs using the latest methodologies.\n\n### 1. The LLM Architecture\n\nYou don‚Äôt need an exhaustive understanding of the Transformer architecture, but it is important to know the major steps in modern LLMs: converting text into numeric tokens, processing these tokens with layers (including attention mechanisms), and using various sampling strategies to generate text.\n\n* Architectural Overview: Trace the evolution from encoder-decoder Transformers to decoder-only structures like GPT, which are fundamental to modern LLMs. Understand how these models process and generate text at a high level.\n* Tokenization: Learn the principles behind tokenization and how it transforms text into numerical data that models can process. Investigate different tokenization strategies and their effects on performance and output quality.\n* Attention Mechanisms: Master the concept of attention, particularly self-attention and its variants, and see how they help models deal with long-range dependencies and maintain contextual integrity.\n* Sampling Techniques: Compare deterministic methods (e.g., greedy search, beam search) to probabilistic methods (e.g., temperature sampling, nucleus sampling) and evaluate the trade-offs involved.\n\nReferences:\n\n* [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown\n* [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft\n* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy (includes a tokenization video: [here](https://www.youtube.com/watch?v=zduSFxRajkE))\n* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng\n* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) by Maxime Labonne\n\n### 2. Pre-training Models\n\nPre-training LLMs is an expensive and resource-intensive process. Although this course does not primarily focus on pre-training, understanding the process, particularly regarding data handling and model parameters, is crucial. For smaller-scale hobbyist projects, pre-training on models with fewer than 1B parameters is feasible.\n\n* Data Preparation: Pre-training requires vast datasets (for example, [Llama 3.1](https://arxiv.org/abs/2307.09288) was trained on 15 trillion tokens), which must be curated, cleaned, deduplicated, and tokenized. Modern pipelines include extensive quality filtering.\n* Distributed Training: Explore techniques such as data parallelism (distributing batches), pipeline parallelism (distributing layers), and tensor parallelism (splitting operations). These require effective network communication and memory management across GPU clusters.\n* Training Optimization: Utilize adaptive learning rate schedules with warm-up, gradient clipping and normalization, mixed-precision training, and modern optimizers (AdamW, Lion) with well-tuned hyperparameters.\n* Monitoring: Implement dashboards and logging to track metrics (loss, gradients, GPU usage) and profile performance to identify computational and communication bottlenecks.\n\nReferences:\n\n* [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) by Penedo et al.\n* [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) by Weber et al.\n* [nanotron](https://github.com/huggingface/nanotron) by Hugging Face (used for [SmolLM2](https://github.com/huggingface/smollm))\n* [Parallel Training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) by Chenyan Xiong\n* [Distributed Training](https://arxiv.org/abs/2407.20018) by Duan et al.\n* [OLMo 2](https://allenai.org/olmo) by AI2\n* [LLM360](https://www.llm360.ai/) by LLM360\n\n### 3. Post-training Datasets\n\nPost-training datasets are organized with clear structures including instructions paired with answers (supervised fine-tuning) or instructions paired with chosen/rejected responses (preference alignment). Given that conversational datasets are less common compared to raw pre-training data, additional processing is often needed to enhance sample accuracy, diversity, and complexity. More details can be found in the [üíæ LLM Datasets](https://github.com/mlabonne/llm-datasets) repository.\n\n* Storage & Chat Templates: Due to their conversational nature, these datasets are stored in formats such as ShareGPT or OpenAI/HF. These are then mapped to chat templates like ChatML or Alpaca for training.\n* Synthetic Data Generation: Use frontier models like GPT-4o to create instruction-response pairs from seed data. This method offers flexibility and scalability, with considerations for diverse seed tasks and effective system prompts.\n* Data Enhancement: Enhance your samples with techniques including verified outputs (using unit tests/solvers), generating multiple answers with rejection sampling, [Auto-Evol](https://arxiv.org/abs/2406.00770), Chain-of-Thought, Branch-Solve-Merge, persona-based approaches, and more.\n* Quality Filtering: Traditional filtering methods involve rule-based approaches, duplicate removal (using MinHash or embeddings), and n-gram decontamination, with reward models and judge LLMs providing additional quality control.\n\nReferences:\n\n* [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) by Argilla\n* [LLM Datasets](https://github.com/mlabonne/llm-datasets) by Maxime Labonne\n* [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) by Nvidia\n* [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) by Argilla\n* [Semhash](https://github.com/MinishLab/semhash) by MinishLab\n* [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) by Hugging Face\n\n### 4. Supervised Fine-Tuning\n\nSupervised Fine-Tuning (SFT) transforms base models into helpful assistants capable of following instructions and structuring answers effectively. Although SFT can be used to introduce new knowledge, its ability to completely learn a new language is limited. Thus, prioritizing data quality over parameter tuning is essential.\n\n* Training Techniques: Full fine-tuning updates all parameters but requires significant computational resources. Techniques like LoRA and QLoRA update only a small number of adapter parameters while keeping the base model frozen. QLoRA further combines 4-bit quantization with LoRA to minimize VRAM usage.\n* Training Parameters: Important parameters to manage include the learning rate (with schedulers), batch size, gradient accumulation, number of epochs, optimizers (e.g., 8-bit AdamW), weight decay, warmup steps, and specific LoRA parameters (rank, alpha, target modules).\n* Distributed Training: Utilize multiple GPUs via frameworks such as DeepSpeed or FSDP. DeepSpeed offers ZeRO optimization stages to improve memory efficiency by partitioning state information. Both frameworks support gradient checkpointing.\n* Monitoring: Keep an eye on metrics like loss curves, learning rate changes, and gradient norms, while addressing issues such as loss spikes or gradient explosions.\n\nReferences:\n\n* [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) by Maxime Labonne\n* [Axolotl ‚Äì Documentation](https://axolotl-ai-cloud.github.io/axolotl/) by Wing Lian\n* [Mastering LLMs](https://parlance-labs.com/education/) by Hamel Husain\n* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka\n\n### 5. Preference Alignment\n\nPreference alignment is a secondary stage in the post-training process that helps fine-tune the model‚Äôs tone and reduce issues like toxicity and hallucinations. Its purpose is to improve performance and usefulness, and it generally involves methods like Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO).\n\n* Rejection Sampling: For each prompt, generate multiple responses from the model, then score them to create on-policy data consisting of both chosen and rejected answers.\n* [Direct Preference Optimization](https://arxiv.org/abs/2305.18290): This method optimizes the policy by directly increasing the likelihood of chosen responses over rejected ones, without needing a separate reward model. Although it is more computationally efficient than PPO, it may offer a slight decrease in quality.\n* [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347): This method iteratively updates the policy to maximize rewards while keeping changes close to the original behavior, using a reward model to score responses and requiring careful hyperparameter tuning (learning rate, batch size, and PPO clip range).\n* Monitoring: Alongside SFT metrics, monitor the margin between chosen and rejected responses and track overall accuracy improvements until reaching a plateau.\n\nReferences:\n\n* [Illustrating RLHF](https://huggingface.co/blog/rlhf) by Hugging Face\n* [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Raschka\n* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face\n* [Fine-tune Mistral-7b with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) by Maxime Labonne\n* [DPO Wandb logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) by Alexander Vishnevskiy\n\n### 6. Evaluation\n\nEvaluating LLMs reliably is a challenging but essential task for refining dataset composition and training settings. It is important to acknowledge Goodhart‚Äôs law: ‚ÄúWhen a measure becomes a target, it ceases to be a good measure.‚Äù\n\n* Automated Benchmarks: Use curated datasets and metrics (such as MMLU) to assess performance on specific tasks. This approach works well for concrete tasks but may struggle with abstract capabilities and suffer from data contamination.\n* Human Evaluation: Involve human assessors to prompt models and rate outputs. This method ranges from informal checks to systematic annotations and large-scale community voting (arena) and tends to work best for subjective assessments.\n* Model-based Evaluation: Implement judge or reward models to assess generated responses. Although they often correlate well with human judgment, these models may be biased toward their own outputs.\n* Feedback Signal: Analyze error patterns to identify shortcomings, such as problems following complex instructions, lacking specific knowledge, or being vulnerable to adversarial prompts. Use the feedback to adjust data generation and training parameters.\n\nReferences:\n\n* [Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook) by Cl√©mentine Fourrier\n* [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) by Hugging Face\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI\n* [Lighteval](https://github.com/huggingface/lighteval) by Hugging Face\n* [Chatbot Arena](https://lmarena.ai/) by LMSYS\n\n### 7. Quantization\n\nQuantization converts a model‚Äôs parameters and activations from high precision (e.g., FP32) to lower precision (such as 4 bits) to reduce compute and memory requirements.\n\n* Base Techniques: Understand the different precisions (FP32, FP16, INT8, etc.) and basic quantization methods like absmax and zero-point techniques.\n* GGUF & llama.cpp: Originally created for CPU-based runs, [llama.cpp](https://github.com/ggerganov/llama.cpp) and the GGUF format are now widely used to run LLMs on consumer hardware. They support the storage of special tokens, vocabulary, and metadata all in one file.\n* GPTQ & AWQ: Methods such as [GPTQ](https://arxiv.org/abs/2210.17323) / [EXL2](https://github.com/turboderp/exllamav2) and [AWQ](https://arxiv.org/abs/2306.00978) use layer-wise calibration to maintain performance at very low bitwidths. These techniques adjust scaling dynamically and can selectively bypass or re-center the heaviest parameters.\n* SmoothQuant & ZeroQuant: New methods such as SmoothQuant (which applies quantization-friendly transformations) and compiler-based optimizations like ZeroQuant help alleviate outlier issues before quantization, optimizing data flow and reducing hardware overhead.\n\nReferences:\n\n* [Introduction to Quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) by Maxime Labonne\n* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) by Maxime Labonne\n* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) by Maxime Labonne\n* [Understanding Activation-Aware Weight Quantization](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI\n* [SmoothQuant on Llama 2 7B](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb) by MIT HAN Lab\n* [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) by DeepSpeed\n\n### 8. New Trends\n\nThis section covers emerging topics that do not neatly fit into other categories. Some ideas, like model merging and multimodal models, are well established, while others‚Äîsuch as interpretability or test-time compute scaling‚Äîare more experimental and actively researched.\n\n* Model Merging: Merging pre-trained models has become a popular technique for boosting performance without additional fine-tuning. The [mergekit](https://github.com/cg123/mergekit) library implements several popular merging methods, including SLERP, [DARE](https://arxiv.org/abs/2311.03099), and [TIES](https://arxiv.org/abs/2311.03099).\n* Multimodal Models: Models like [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), and [LLaVA](https://llava-vl.github.io/) are designed to process and integrate various types of inputs (text, images, audio, etc.) within a unified embedding space, enabling powerful applications such as text-to-image generation.\n* Interpretability: Mechanistic interpretability approaches, including Sparse Autoencoders (SAEs) and techniques like abliteration, offer insights into the internal operations of LLMs and can allow for behavioral adjustments without retraining.\n* Test-time Compute: Scaling computational resources during inference often requires multiple calls and specialized models (e.g., Process Reward Model (PRM)). Iterative procedures with fine-tuned scoring can markedly enhance performance on complex reasoning tasks.\n\nReferences:\n\n* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) by Maxime Labonne\n* [Smol Vision](https://github.com/merveenoyan/smol-vision) by Merve Noyan\n* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen\n* [Uncensor any LLM with abliteration](https://huggingface.co/blog/mlabonne/abliteration) by Maxime Labonne\n* [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) by Adam Karvonen\n* [Scaling test-time compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) by Beeching et al.\n\n### The LLM Engineer\n\nThis part of the course teaches you how to build production-grade applications powered by LLMs, with a focus on augmenting models and deploying them.\n\n### 1. Running LLMs\n\nRunning LLMs can be challenging given their high hardware requirements. Depending on your needs, you might opt to use an API (like GPT-4) or run a model locally. In either case, careful prompting and guidance can greatly enhance output quality and relevance.\n\n* LLM APIs: APIs provide a convenient way to access LLMs. They are divided between private LLMs (e.g., [OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), [Cohere](https://docs.cohere.com/docs)) and open-source LLMs (e.g., [OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/)).\n* Open-source LLMs: The [Hugging Face Hub](https://huggingface.co/models) is a prime resource for finding LLMs. You can run many of these models in [Hugging Face Spaces](https://chatgpt.com/), or download and operate them locally using tools like [LM Studio](https://lmstudio.ai/), [llama.cpp](https://github.com/ggerganov/llama.cpp), or [Ollama](https://ollama.ai/).\n* Prompt Engineering: Techniques such as zero-shot prompting, few-shot prompting, chain-of-thought, and ReAct are common. While these methods work better with larger models, they can be adapted for smaller ones.\n* Structuring Outputs: Some tasks require outputs to follow a strict format (such as a JSON format or specific template). Tools such as [LMQL](https://lmql.ai/), [Outlines](https://github.com/outlines-dev/outlines), and [Guidance](https://github.com/guidance-ai/guidance) help ensure the generated text adheres to the required structure.\n\nReferences:\n\n* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya\n* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI\n* [Outlines ‚Äì Quickstart](https://outlines-dev.github.io/outlines/quickstart/)\n* [LMQL ‚Äì Overview](https://lmql.ai/docs/language/overview.html)\n\n### 2. Building a Vector Storage\n\nThe first step in creating a Retrieval Augmented Generation (RAG) pipeline is establishing a vector storage. This involves loading documents, splitting them into manageable pieces, and then converting key text chunks into vector embeddings for future retrieval.\n\n* Ingesting Documents: Document loaders can process multiple formats such as PDF, JSON, HTML, and Markdown. They can also pull in data directly from databases and APIs (e.g., GitHub, Reddit, Google Drive).\n* Splitting Documents: Text splitters divide documents into smaller, semantically relevant chunks. Instead of a fixed character count, splitting by headers or recursively‚Äîwhile preserving metadata‚Äîoften yields better results.\n* Embedding Models: These models transform text into vector representations, enabling a more nuanced semantic interpretation that is essential for effective search.\n* Vector Databases: Databases like [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), and [Annoy](https://github.com/spotify/annoy) are designed for storing embeddings, allowing for fast similarity-based retrieval.\n\nReferences:\n\n* [LangChain ‚Äì Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n* [Sentence Transformers library](https://www.sbert.net/)\n* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n* [The Top 5 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali\n\n### 3. Retrieval Augmented Generation\n\nRetrieval Augmented Generation (RAG) enhances LLM outputs by using relevant contextual documents fetched from a vector database, thus improving answer accuracy without needing additional fine-tuning.\n\n* Orchestrators: Tools like [LangChain](https://python.langchain.com/docs/get_started/introduction), [LlamaIndex](https://docs.llamaindex.ai/en/stable/), and [FastRAG](https://github.com/IntelLabs/fastRAG) connect LLMs to tools, databases, and memory systems, extending their functionality.\n* Retrievers: Since user queries may not be optimized for search, techniques such as multi-query retrievers or [HyDE](https://arxiv.org/abs/2212.10496) can reformulate queries to improve retrieval performance.\n* Memory: To maintain context over a conversation, LLMs use a history buffer that can be enhanced with summarization techniques or integrated with vector stores via RAG.\n* Evaluation: It is crucial to assess both the document retrieval process (precision and recall) and the generation stage (faithfulness and relevancy). Tools like [Ragas](https://github.com/explodinggradients/ragas/tree/main) and [DeepEval](https://github.com/confident-ai/deepeval) can assist in these evaluations.\n\nReferences:\n\n* [Llamaindex ‚Äì High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html)\n* [Pinecone ‚Äì Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/)\n* [LangChain ‚Äì Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart)\n* [LangChain ‚Äì Memory types](https://python.langchain.com/docs/modules/memory/types/)\n* [RAG pipeline ‚Äì Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html)\n\n### 4. Advanced RAG\n\nIn real-world scenarios, you may need to develop more complex pipelines involving SQL or graph databases, as well as systems that automatically select appropriate tools and APIs to enhance the baseline RAG setup.\n\n* Query Construction: For structured data stored in databases, you need to translate user instructions into appropriate query languages like SQL or Cypher.\n* Agents and Tools: LLM agents can automatically choose the most suitable tools‚Äîranging from simple web searches (e.g., Google, Wikipedia) to complex systems (e.g., Python interpreters, Jira)‚Äîto answer queries.\n* Post-Processing: Enhance the overall relevance of retrieved documents using re-ranking methods, [RAG-fusion](https://github.com/Raudaschl/rag-fusion), or classification techniques.\n* Program LLMs: Frameworks like [DSPy](https://github.com/stanfordnlp/dspy) allow you to fine-tune prompts and model parameters programmatically based on automated evaluations.\n\nReferences:\n\n* [LangChain ‚Äì Query Construction](https://blog.langchain.dev/query-construction/)\n* [LangChain ‚Äì SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql)\n* [Pinecone ‚Äì LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/)\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng\n* [LangChain ‚Äì OpenAI‚Äôs RAG](https://blog.langchain.dev/applying-openai-rag/)\n* [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task)\n\n### 5. Inference Optimization\n\nSince generating text is computationally intensive, several techniques exist to maximize throughput and reduce inference costs alongside quantization.\n\n* Flash Attention: Optimizes the attention mechanism by reducing its complexity from quadratic to linear, thereby speeding up both training and inference.\n* Key-value Cache: Learn about the key-value cache and enhancements like [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).\n* Speculative Decoding: Use a smaller model to produce draft outputs that are later refined by a larger model, thus accelerating text generation.\n\nReferences:\n\n* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face\n* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks\n* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face\n* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face\n\n### 6. Deploying LLMs\n\nDeploying LLMs, especially at scale, is complex and may require multiple GPU clusters. However, demos or local applications often have simpler requirements.\n\n* Local Deployment: Open-source LLMs offer privacy advantages over private models. Solutions such as [LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), and [kobold.cpp](https://github.com/LostRuins/koboldcpp) facilitate local deployment.\n* Demo Deployment: Tools like [Gradio](https://www.gradio.app/) and [Streamlit](https://docs.streamlit.io/) are excellent for prototyping apps and sharing demos. They are also easy to host online (for example, on [Hugging Face Spaces](https://huggingface.co/spaces)).\n* Server Deployment: Running LLMs at scale often demands cloud infrastructure (or on-prem solutions) and specialized frameworks such as [TGI](https://github.com/huggingface/text-generation-inference) or [vLLM](https://github.com/vllm-project/vllm/tree/main).\n* Edge Deployment: In resource-constrained environments, frameworks like [MLC LLM](https://github.com/mlc-ai/mlc-llm) and [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) enable deployment on web browsers, Android, and iOS.\n\nReferences:\n\n* [Streamlit ‚Äì Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps) by Streamlit\n* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm) by Hugging Face\n* [Philschmid blog](https://www.philschmid.de/) by Philipp Schmid\n* [Optimizing Latency](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain\n\n### 7. Securing LLMs\n\nLLM applications bring their own unique security challenges in addition to standard software vulnerabilities.\n\n* Prompt Hacking: This includes issues like prompt injection (where unwanted instructions hijack the model), data/prompt leaking (extracting the original prompt or training data), and jailbreaking (bypassing the model‚Äôs safety features).\n* Backdoors: These attacks can target training data by poisoning it with false or malicious content, or by introducing hidden triggers that alter model behavior during inference.\n* Defensive Measures: Protect your LLM applications by testing them for vulnerabilities using techniques such as red teaming and tools like [garak](https://github.com/leondz/garak/), while monitoring in production with frameworks like [langfuse](https://github.com/langfuse/langfuse).\n\nReferences:\n\n* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki\n* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker\n* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec)\n* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft",
  "metadata": {
    "url": "https://originshq.com/blog/top-ai-llm-learning-resource-in-2025/#ib-toc-anchor-0",
    "crawled_at": "2025-02-28T10:04:53.877168"
  }
}