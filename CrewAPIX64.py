import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
from mistralai import Mistral
from dotenv import load_dotenv

# à¹‚à¸«à¸¥à¸”à¸„à¹ˆà¸² API Key à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ .env
load_dotenv()
api_key = os.getenv("MISTRAL_API_KEY")

if not api_key:
    print("âŒ MISTRAL_API_KEY not found in .env file")
    print("ğŸ“ Please add your Mistral API key to .env file:")
    print("MISTRAL_API_KEY=your_api_key_here")
    exit(1)

try:
    client = Mistral(api_key=api_key)
except Exception as e:
    print(f"âŒ Error initializing Mistral client: {str(e)}")
    print("âš ï¸ Please check if your API key is valid")
    exit(1)

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸à¸²à¸£à¹€à¸Šà¹‡à¸„ robots.txt
def check_robots_txt(url):
    try:
        response = requests.get(url + "/robots.txt")
        response.raise_for_status()
        return "Disallow" not in response.text
    except Exception as e:
        print(f"Error checking robots.txt: {e}")
        return True  # Assume allowed if error

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸‚à¸¹à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¹€à¸§à¹‡à¸šà¹„à¸‹à¸•à¹Œ
def scrape_content(url):
    print(f"\nğŸ” Starting to scrape: {url}")
    
    if not check_robots_txt(url):
        print("âŒ Crawling disallowed by robots.txt")
        return None

    try:
        print("ğŸ“¡ Fetching page content...")
        response = requests.get(url)
        response.raise_for_status()

        # Check if it's a raw markdown file
        if url.endswith('.md'):
            print("ğŸ“ Processing raw markdown content...")
            content = []
            
            # Split the content into lines and process
            lines = response.text.split('\n')
            for line in lines:
                line = line.strip()
                if line:  # Skip empty lines
                    # Handle headers
                    if line.startswith('#'):
                        content.append(line)
                    # Handle lists
                    elif line.startswith('- ') or line.startswith('* '):
                        content.append(line)
                    # Handle everything else
                    else:
                        content.append(line)
            
            return {
                "title": "README.md",
                "text": "\n\n".join(content),
                "metadata": {
                    "url": url,
                    "crawled_at": datetime.now().isoformat()
                }
            }

        # Handle HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Handle GitHub repository pages
        if 'github.com' in url and '/raw/' not in url:
            print("ğŸ” Detected GitHub repository, parsing specific elements...")
            
            # Get repository title
            title_elem = soup.find('strong', {'itemprop': 'name'})
            if not title_elem:
                title_elem = soup.find('h1', class_='d-flex')
            title = title_elem.text.strip() if title_elem else "No Title Found"
            print(f"ğŸ“š Found repository title: {title}")

            # Get repository content
            content = []
            
            # Get repository description
            description = soup.find('p', {'class': 'f4'})
            if description:
                desc_text = description.text.strip()
                content.append(desc_text)
                print(f"ğŸ“‹ Found repository description: {desc_text}")

            # Get README content from the article directly
            readme = soup.find('article', class_='markdown-body')
            
            if readme:
                print("ğŸ“– Extracting README content...")
                
                # Remove unwanted elements
                for unwanted in readme.find_all(['nav', 'footer', 'script', 'style']):
                    unwanted.decompose()
                
                # Extract content sequentially maintaining structure
                for element in readme.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):
                    if element.name.startswith('h'):
                        header_text = element.get_text(strip=True)
                        if header_text:
                            content.append(f"{'#' * int(element.name[1])} {header_text}")
                    elif element.name == 'p':
                        text = element.get_text(strip=True)
                        if text:
                            content.append(text)
                    elif element.name in ['ul', 'ol']:
                        for li in element.find_all('li', recursive=False):
                            li_text = li.get_text(strip=True)
                            if li_text:
                                content.append(f"- {li_text}")
                
                print(f"[+] Successfully extracted {len(content)} content blocks from README")
            else:
                print("[-] No README content found. Trying alternative method...")
                
                # Try finding the README content in the Box-body
                readme_box = soup.find('div', class_='Box-body')
                if readme_box:
                    print("[*] Found README in Box-body")
                    
                    # Remove unwanted elements
                    for unwanted in readme_box.find_all(['nav', 'footer', 'script', 'style']):
                        unwanted.decompose()
                    
                    # Extract meaningful content
                    for element in readme_box.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):
                        if element.parent.name not in ['nav', 'footer']:
                            if element.name.startswith('h'):
                                header_text = element.get_text(strip=True)
                                if header_text:
                                    content.append(f"{'#' * int(element.name[1])} {header_text}")
                            elif element.name == 'p':
                                text = element.get_text(strip=True)
                                if text:
                                    content.append(text)
                            elif element.name in ['ul', 'ol']:
                                for li in element.find_all('li', recursive=False):
                                    li_text = li.get_text(strip=True)
                                    if li_text:
                                        content.append(f"- {li_text}")
                    
                    print(f"[+] Successfully extracted {len(content)} content blocks from Box-body")

        else:
            print("[*] Processing non-GitHub webpage...")
            # Handle other websites as before
            title = soup.find('h1').text.strip() if soup.find('h1') else "No Title Found"
            content = []
            article = soup.find('article') or soup.find(class_='entry-content')
            if article:
                content = extract_content_from_element(article)

        result = {
            "title": title,
            "text": "\n\n".join(content),
            "metadata": {
                "url": url,
                "crawled_at": datetime.now().isoformat()
            }
        }
        
        print(f"[+] Successfully scraped content:")
        print(f"    - Title: {title}")
        print(f"    - Content length: {len(result['text'])} characters")
        return result

    except Exception as e:
        print(f"[-] Error scraping content: {str(e)}")
        return None

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹ƒà¸™à¸à¸²à¸£à¹à¸¢à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ HTML
def extract_content_from_element(element):
    content = []
    for child in element.children:
        if child.name == 'p':
            content.append(child.get_text(strip=True))
        elif child.name in ['h2', 'h3', 'h4', 'h5', 'h6']:
            content.append(f"{'#' * int(child.name[1])} {child.get_text(strip=True)}")
        elif child.name == 'ul' or child.name == 'ol':
            for li in child.find_all('li'):
                content.append(f"- {li.get_text(strip=True)}")
        elif child.string:
            content.append(child.string.strip())
    return content

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸ªà¸£à¸¸à¸›à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Mistral API
def split_content(content, max_chars=20000):
    """Split content into chunks of max_chars, breaking at newlines when possible."""
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in content.split('\n'):
        line_length = len(line)
        
        if current_length + line_length + 1 <= max_chars:  # +1 for newline
            current_chunk.append(line)
            current_length += line_length + 1
        else:
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
            current_chunk = [line]
            current_length = line_length
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks

def summarize_with_mistral(content):
    print("\nğŸ¤– Starting content summarization with Mistral AI...")
    try:
        print(f"ğŸ“Š Total content length: {len(content)} characters")
        
        # Split content into chunks if it's too large
        if len(content) > 20000:
            print("ğŸ“ Content too large, splitting into chunks...")
            chunks = split_content(content)
            print(f"ğŸ”„ Split content into {len(chunks)} chunks")
            
            # Summarize each chunk with progress tracking
            summaries = []
            for i, chunk in enumerate(chunks, 1):
                print(f"\nğŸ”„ Processing chunk {i}/{len(chunks)} ({len(chunk)} characters)")
                try:
                    print(f"â³ Generating summary for chunk {i}...")
                    response = client.chat.complete(
                        model="mistral-large-latest",
                        messages=[
                            {
                                "role": "user",
                                "content": f"Please summarize the following content concisely:\n\n{chunk}"
                            }
                        ]
                    )
                    chunk_summary = response.choices[0].message.content
                    summaries.append(chunk_summary)
                    print(f"âœ… Successfully summarized chunk {i}")
                except Exception as e:
                    print(f"âŒ Error summarizing chunk {i}: {str(e)}")
                    summaries.append(f"[Summary failed for chunk {i}]")
            
            # If we have multiple summaries, combine them
            if len(summaries) > 1:
                print("\nğŸ”„ Combining chunk summaries...")
                try:
                    # Check if any chunks failed
                    failed_chunks = [i+1 for i, s in enumerate(summaries) if "[Summary failed for chunk" in s]
                    if failed_chunks:
                        print(f"âš ï¸ Note: Chunks {', '.join(map(str, failed_chunks))} failed to summarize")
                    
                    final_summary_prompt = "Please provide a concise overall summary of these summaries:\n\n"
                    final_summary_prompt += "\n\n".join([f"Summary {i+1}:\n{s}" for i, s in enumerate(summaries)])
                    
                    print("â³ Generating final summary...")
                    response = client.chat.complete(
                        model="mistral-large-latest",
                        messages=[
                            {
                                "role": "user",
                                "content": final_summary_prompt
                            }
                        ]
                    )
                    summary = response.choices[0].message.content
                    print("âœ¨ Final summary generated successfully!")
                except Exception as e:
                    print(f"âŒ Error generating final summary: {str(e)}")
                    summary = "âš ï¸ Failed to generate final summary. Individual chunk summaries:\n\n" + "\n\n".join(summaries)
            else:
                summary = summaries[0]
                if "[Summary failed for chunk" in summary:
                    summary = "âš ï¸ " + summary
        else:
            # For smaller content, summarize directly
            response = client.chat.complete(
                model="mistral-large-latest",
                messages=[
                    {
                        "role": "user",
                        "content": f"Please summarize the following content concisely:\n\n{content}"
                    }
                ]
            )
            summary = response.choices[0].message.content
        
        print("[+] Successfully generated summary")
        print(f"[*] Final summary length: {len(summary)} characters")
        return summary
    except Exception as e:
        error_message = str(e)
        print(f"âŒ Error with Mistral summarization: {error_message}")
        
        if "api_key" in error_message.lower():
            return "âš ï¸ AI Summarization failed: Missing or invalid API key. Please check your MISTRAL_API_KEY in .env file."
        elif "timeout" in error_message.lower():
            return "âš ï¸ AI Summarization failed: Request timed out. Please try again."
        elif "too many requests" in error_message.lower():
            return "âš ï¸ AI Summarization failed: Rate limit exceeded. Please wait a moment and try again."
        else:
            return f"âš ï¸ AI Summarization failed: {error_message}"

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹ƒà¸™à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸š JSON
def save_content_json(content, output_file):
    print(f"\nğŸ’¾ Saving content to JSON file: {output_file}")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(content, f, indent=2, ensure_ascii=False)
    print(f"âœ… Successfully saved JSON file")

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹ƒà¸™à¸à¸²à¸£à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸š Markdown
def save_content_markdown(content, output_file):
    print(f"\nğŸ“ Saving content to Markdown file: {output_file}")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# {content['title']}\n\n")
        f.write(f"URL: {content['metadata']['url']}\n\n")
        f.write(f"Crawled at: {content['metadata']['crawled_at']}\n\n")
        f.write(f"## Summary\n\n{content['summary']}\n\n")
        f.write(content['text'])
    print(f"âœ… Successfully saved Markdown file")

def print_ascii_art():
    ascii_art = """
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—
â•šâ•â•â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
 â–ˆâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ•‘
â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•      â•šâ•â•
    """
    print(ascii_art)

# à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸«à¸¥à¸±à¸
def main():
    print_ascii_art()
    print("\nğŸš€ === Web Scraping and Summarization Tool === ğŸš€")
    
    # Prompt for repository URL
    print("\nğŸ“Œ Enter GitHub repository URL (or press Enter for default):")
    user_input = input("â¡ï¸ ").strip()
    
    # Use default if no input provided
    repo_url = user_input if user_input else "https://example.com"
    
    raw_url = f"{repo_url}/raw/master/README.md"
    print(f"\nğŸ”— Repository URL: {repo_url}")
    print(f"[*] Fetching README from: {raw_url}")
    
    # First try the raw README URL
    content = scrape_content(raw_url)
    if not content or not content.get('text'):
        print("[-] Raw README not found, trying main branch...")
        raw_url = f"{repo_url}/raw/main/README.md"
        content = scrape_content(raw_url)
        
    # If still no content, fall back to repository page
    if not content or not content.get('text'):
        print("[-] Failed to fetch raw README, falling back to repository page...")
        content = scrape_content(repo_url)

    if content:
        print("\n[*] Processing scraped content...")
        # Summarize content using Mistral API
        summary = summarize_with_mistral(content['text'])

        # Add summary to content
        content['summary'] = summary

        # Save content in JSON and Markdown formats
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = f"scraped_output/scraped_{timestamp}.json"
        md_file = f"scraped_output/scraped_{timestamp}.md"
        
        # Ensure output directory exists
        os.makedirs("scraped_output", exist_ok=True)
        
        save_content_json(content, json_file)
        save_content_markdown(content, md_file)
        
        print("\nğŸ‰ Operation completed successfully:")
        print(f"    ğŸ“„ JSON output: {json_file}")
        print(f"    ğŸ“ Markdown output: {md_file}")
    else:
        print("\nâŒ Failed to crawl content")

if __name__ == "__main__":
    main()
