# Display ASCII Art

ascii_art = """

███████╗ ██████╗ ███╗   ███╗██████╗ ██╗████████╗██╗  ██╗ ██████╗ ██╗  ██╗
╚══███╔╝██╔═══██╗████╗ ████║██╔══██╗██║╚══██╔══╝╚██╗██╔╝██╔════╝ ██║  ██║
  ███╔╝ ██║   ██║██╔████╔██║██████╔╝██║   ██║    ╚███╔╝ ███████╗ ███████║
 ███╔╝  ██║   ██║██║╚██╔╝██║██╔══██╗██║   ██║    ██╔██╗ ██╔═══██╗╚════██║
███████╗╚██████╔╝██║ ╚═╝ ██║██████╔╝██║   ██║   ██╔╝ ██╗╚██████╔╝     ██║

"""

print('\033[95m' + ascii_art + '\033[0m')  # Print in purple color
print("🚀 ZombitX64 Web Scraper v1.2.3 - Enhanced Legal Compliance")

This notebook demonstrates:

1. 🔗 Connecting to Google Drive
2. 📦 Setting up Hugging Face transformers
3. 🌐 Web scraping with legal compliance
4. 🤖 Local model inference
5. 💾 Saving results to Google Drive
6. 🔒 Enhanced legal protections

## Setup and Dependencies
!pip install transformers beautifulsoup4 requests

!pip install torch torchvision torchaudio

## Connect to Google Drive
from google.colab import drive

drive.mount('/content/drive')

# Create output directory in Google Drive

import os

output_dir = '/content/drive/MyDrive/ZombitX64_Output'

os.makedirs(output_dir, exist_ok=True)

print(f"✅ Output directory created at: {output_dir}")

## Import Required Libraries
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqGeneration
from bs4 import BeautifulSoup
import requests
import json
from datetime import datetime
import re
import urllib.parse
import logging
from urllib.robotparser import RobotFileParser

## Security Manager Class
class SecurityManager:
    def __init__(self):
        self.robot_parsers = {}
        self.warnings_shown = set()
    
    def show_warning(self, warning_type, url=None):
        """Show a warning message only once per type/url combination"""
        warning_key = f"{warning_type}:{url}" if url else warning_type
        if warning_key not in self.warnings_shown:
            warnings = {
                'tos': "\n⚠️ WARNING: Please review the website's Terms of Service before scraping data.\n"
                      "Make sure your use complies with their policies.",
                'gdpr': "\n⚠️ GDPR WARNING: Ensure you have proper consent and legal basis for collecting\n"
                       "and storing personal data. Follow GDPR requirements for data processing.",
                'copyright': "\n⚠️ COPYRIGHT WARNING: Verify that you have the right to use and store\n"
                           "the content. Respect intellectual property rights.",
                'pdf': "\n⚠️ PDF WARNING: Before downloading or processing PDFs, ensure they are publicly\n"
                      "available or you have permission to access them."
            }
            if warning_type in warnings:
                print(warnings[warning_type])
                self.warnings_shown.add(warning_key)
    
    def check_robots_txt(self, url, user_agent):
        """Check if URL is allowed by robots.txt"""
        parsed = urllib.parse.urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        
        if robots_url not in self.robot_parsers:
            rp = RobotFileParser()
            rp.set_url(robots_url)
            try:
                response = requests.get(robots_url, timeout=5)
                if response.status_code == 200:
                    rp.parse(response.text.splitlines())
                else:
                    print(f"⚠️ Could not fetch robots.txt ({response.status_code}), assuming crawling is allowed")
                    rp.allow_all = True
            except Exception as e:
                print(f"⚠️ Error fetching robots.txt: {e}, assuming crawling is allowed")
                rp.allow_all = True
            self.robot_parsers[robots_url] = rp
        
        return self.robot_parsers[robots_url].can_fetch(user_agent, url)
    
    def can_scrape(self, url, user_agent):
        """Check if scraping is allowed for this URL"""
        # Show Terms of Service warning for new domains
        parsed = urllib.parse.urlparse(url)
        domain = parsed.netloc
        self.show_warning('tos', domain)
        
        # Check robots.txt
        is_allowed = self.check_robots_txt(url, user_agent)
        if not is_allowed:
            print(f"\n🚫 Scraping not allowed by robots.txt for URL: {url}")
        return is_allowed

# Initialize security manager
security_manager = SecurityManager()

## Load Local Hugging Face Model
# Load model and tokenizer

model_name = "facebook/bart-large-cnn"  # You can change this to other models

print(f"🤖 Loading model: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqGeneration.from_pretrained(model_name)

# Move model to GPU if available

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = model.to(device)

print(f"✅ Model loaded successfully on: {device}")

## Web Scraping Functions
def check_legal_compliance(url):
    """Check legal compliance for the URL"""
    print("\n🔒 Checking legal compliance...")
    
    # Check if we can scrape this URL
    if not security_manager.can_scrape(url, "CrewColabX64/1.2.3"):
        print("🚫 URL is not allowed by robots.txt")
        return False
    
    # Show GDPR warning
    security_manager.show_warning('gdpr')
    
    # Show copyright warning
    security_manager.show_warning('copyright')
    
    print("✅ Legal compliance checks complete")
    return True

def scrape_content(url):
    print(f"\n🔍 Starting to scrape: {url}")
    
    # Perform legal compliance checks
    if not check_legal_compliance(url):
        return None

    try:
        print("📡 Fetching content...")
        response = requests.get(url)
        response.raise_for_status()

        # Check if it's a PDF
        if '.pdf' in url.lower() or response.headers.get('content-type', '').lower() == 'application/pdf':
            security_manager.show_warning('pdf')

        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract title and content
        title = soup.find('h1').text.strip() if soup.find('h1') else "No Title Found"

        content = []
        # Find main content
        article = soup.find('article') or soup.find(class_='entry-content')

        if article:
            for elem in article.find_all(['p', 'h2', 'h3', 'ul', 'ol']):
                content.append(elem.get_text(strip=True))

        result = {
            "title": title,
            "text": "\n\n".join(content),
            "metadata": {
                "url": url,
                "crawled_at": datetime.now().isoformat()
            }
        }

        # Check for personal data or copyrighted content
        text_content = result['text'].lower()
        if any(term in text_content for term in ['email', 'phone', 'address', 'name']):
            security_manager.show_warning('gdpr')
        if any(term in text_content for term in ['copyright', '©', 'all rights reserved']):
            security_manager.show_warning('copyright')

        print(f"✅ Successfully scraped content ({len(result['text'])} characters)")
        return result

    except Exception as e:
        print(f"❌ Error scraping content: {str(e)}")
        return None

## Text Summarization Function
def summarize_text(text, max_length=1024):
    try:
        print("\n🤖 Generating summary...")
        print("📊 Processing text structure...")

        inputs = tokenizer(text, max_length=max_length, truncation=True, return_tensors="pt").to(device)
        
        # Show progress indicator
        print("⏳ Running model inference...")

        # Generate summary
        summary_ids = model.generate(
            inputs["input_ids"],
            max_length=150,
            min_length=40,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
        
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print("✨ Summary generated successfully!")
        return summary

    except Exception as e:
        print(f"❌ Error generating summary: {str(e)}")
        return "Summary generation failed."

## Save Results to Google Drive
def save_results(content, summary):
    print("\n💾 Saving results to Google Drive...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save JSON
    print("📊 Preparing JSON output...")
    json_path = f"{output_dir}/scraped_{timestamp}.json"
    content_with_summary = {
        **content,
        "summary": summary
    }
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(content_with_summary, f, indent=2, ensure_ascii=False)
    
    # Save Markdown
    print("📝 Preparing Markdown output...")
    md_path = f"{output_dir}/scraped_{timestamp}.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(f"# {content['title']}\n\n")
        f.write(f"URL: {content['metadata']['url']}\n\n")
        f.write(f"Crawled at: {content['metadata']['crawled_at']}\n\n")
        f.write(f"## Summary\n\n{summary}\n\n")
        f.write(content['text'])
    
    print(f"\n✅ Results saved successfully:")
    print(f"    📄 JSON: {json_path}")
    print(f"    📝 Markdown: {md_path}")

## Main Execution
print("\n=== Web Scraping and Summarization Tool ===")
print("Version 1.2.3 - Enhanced Legal Compliance")

# Input URL
url = input("🌐 Enter the URL to scrape (or press Enter for default): ") or "https://example.com"

# Scrape content
content = scrape_content(url)

if content:
    # Generate summary
    summary = summarize_text(content['text'])
    
    # Save results
    save_results(content, summary)
    
    # Show completion message
    print('\n' + '\033[95m' + ascii_art + '\033[0m')
    print("\n🎉 Task completed successfully!")
    print("🔒 Legal compliance checks performed")
    print("📝 Content processed and saved")
else:
    print("\n❌ Failed to scrape content")
