{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ASCII Art\n",
    "ascii_art = \"\"\"\n",
    "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—\n",
    "â•šâ•â•â–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘\n",
    "  â–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘\n",
    " â–ˆâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘    â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ•â•â•â•â–ˆâ–ˆâ•‘\n",
    "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•     â–ˆâ–ˆâ•‘\n",
    "â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•      â•šâ•â•\n",
    "\"\"\"\n",
    "print('\\033[95m' + ascii_art + '\\033[0m')  # Print in purple color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ZombitX64 Web Scraper with Local Hugging Face Models\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. ğŸ”— Connecting to Google Drive\n",
    "2. ğŸ“¦ Setting up Hugging Face transformers\n",
    "3. ğŸŒ Web scraping functionality\n",
    "4. ğŸ¤– Local model inference\n",
    "5. ğŸ’¾ Saving results to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers beautifulsoup4 requests\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "import os\n",
    "output_dir = '/content/drive/MyDrive/ZombitX64_Output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"âœ… Output directory created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqGeneration\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Local Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"  # You can change this to other models\n",
    "print(f\"ğŸ¤– Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"âœ… Model loaded successfully on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_robots_txt(url):\n",
    "    try:\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        if not parsed_url.netloc:\n",
    "            parsed_url = urllib.parse.urlparse(\"http://\" + url)\n",
    "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "        response = requests.get(robots_url)\n",
    "        response.raise_for_status()\n",
    "        return \"Disallow: /\" not in response.text\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error checking robots.txt: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def scrape_content(url):\n",
    "    if not check_robots_txt(url):\n",
    "        print(\"âŒ Crawling disallowed by robots.txt\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"ğŸŒ Fetching content from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract title and content\n",
    "        title = soup.find('h1').text.strip() if soup.find('h1') else \"No Title Found\"\n",
    "        content = []\n",
    "\n",
    "        # Find main content\n",
    "        article = soup.find('article') or soup.find(class_='entry-content')\n",
    "        if article:\n",
    "            for elem in article.find_all(['p', 'h2', 'h3', 'ul', 'ol']):\n",
    "                content.append(elem.get_text(strip=True))\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"text\": \"\\n\\n\".join(content),\n",
    "            \"metadata\": {\n",
    "                \"url\": url,\n",
    "                \"crawled_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error scraping content: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_length=1024):\n",
    "    try:\n",
    "        print(\"ğŸ¤– Generating summary...\")\n",
    "        inputs = tokenizer(text, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        print(\"âœ… Summary generated successfully!\")\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating summary: {str(e)}\")\n",
    "        return \"Summary generation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(content, summary):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = f\"{output_dir}/scraped_{timestamp}.json\"\n",
    "    content_with_summary = {\n",
    "        **content,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_with_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save Markdown\n",
    "    md_path = f\"{output_dir}/scraped_{timestamp}.md\"\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {content['title']}\\n\\n\")\n",
    "        f.write(f\"URL: {content['metadata']['url']}\\n\\n\")\n",
    "        f.write(f\"Crawled at: {content['metadata']['crawled_at']}\\n\\n\")\n",
    "        f.write(f\"## Summary\\n\\n{summary}\\n\\n\")\n",
    "        f.write(content['text'])\n",
    "    \n",
    "    print(f\"âœ… Results saved to Google Drive:\")\n",
    "    print(f\"ğŸ“„ JSON: {json_path}\")\n",
    "    print(f\"ğŸ“ Markdown: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input URL\n",
    "url = input(\"ğŸŒ Enter the URL to scrape (or press Enter for default): \") or \"https://example.com\"\n",
    "\n",
    "# Scrape content\n",
    "content = scrape_content(url)\n",
    "\n",
    "if content:\n",
    "    # Generate summary\n",
    "    summary = summarize_text(content['text'])\n",
    "    \n",
    "    # Save results\n",
    "    save_results(content, summary)\n",
    "    \n",
    "    # Show ASCII art again for completion\n",
    "    print('\\n\\033[95m' + ascii_art + '\\033[0m')\n",
    "    print(\"âœ¨ Task completed successfully! âœ¨\")\n",
    "else:\n",
    "    print(\"âŒ Failed to scrape content\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ZombitX64_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
