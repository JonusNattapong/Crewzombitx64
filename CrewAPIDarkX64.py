import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os
from mistralai import Mistral
from dotenv import load_dotenv

# Set ASCII colors for dark mode
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'
    PURPLE = '\033[35m'
    CYAN = '\033[36m'

# Load API Key from .env
load_dotenv()
api_key = os.getenv("MISTRAL_API_KEY")

def print_disclaimer():
    """Print disclaimer and warning message"""
    print(f"\n{Colors.WARNING}‚ö†Ô∏è DISCLAIMER AND WARNING / ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö ‚ö†Ô∏è{Colors.ENDC}")
    print(f"{Colors.WARNING}=" * 80)
    print(f"{Colors.BOLD}üîí [EN] IMPORTANT DISCLAIMER:{Colors.ENDC}")
    print("This tool is for educational purposes only.")
    print("Users are fully responsible for their own actions.")
    print("The developers assume no liability and are not responsible for any misuse or damage.")
    print("\nüö´ By using this tool, you agree to:")
    print("1. Use it legally and ethically")
    print("2. Respect website terms of service")
    print("3. Not use for malicious purposes")
    print("4. Take full responsibility for your actions")
    
    print(f"\n{Colors.BOLD}üîí [TH] ‡∏Ñ‡∏≥‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:{Colors.ENDC}")
    print("‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô")
    print("‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏ô‡πÄ‡∏≠‡∏á")
    print("‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡∏¢‡∏´‡∏≤‡∏¢‡πÉ‡∏î‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ô‡∏µ‡πâ")
    print("\n‚ö†Ô∏è ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:")
    print("1. ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏¢‡∏ò‡∏£‡∏£‡∏°")
    print("2. ‡πÄ‡∏Ñ‡∏≤‡∏£‡∏û‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå")
    print("3. ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢")
    print("4. ‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î")
    print(f"\n{Colors.FAIL}‚ùó ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ú‡∏¥‡∏î‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‚ùó{Colors.ENDC}")
    print(f"{Colors.WARNING}=" * 80 + f"{Colors.ENDC}\n")

def print_dark_ascii():
    """Print ASCII art in dark mode colors"""
    ascii_art = f"""{Colors.PURPLE}
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó
‚ïö‚ïê‚ïê‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
  ‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë    ‚ïö‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë
 ‚ñà‚ñà‚ñà‚ïî‚ïù  ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ïê‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù     ‚ñà‚ñà‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù     ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù      ‚ïö‚ïê‚ïù
{Colors.ENDC}"""
    print(ascii_art)

if not api_key:
    print(f"{Colors.FAIL}‚ùå MISTRAL_API_KEY not found in .env file{Colors.ENDC}")
    print(f"{Colors.WARNING}üìù Please add your Mistral API key to .env file:{Colors.ENDC}")
    print("MISTRAL_API_KEY=your_api_key_here")
    exit(1)

try:
    client = Mistral(api_key=api_key)
except Exception as e:
    print(f"{Colors.FAIL}‚ùå Error initializing Mistral client: {str(e)}{Colors.ENDC}")
    print(f"{Colors.WARNING}‚ö†Ô∏è Please check if your API key is valid{Colors.ENDC}")
    exit(1)

def extract_repository_data(soup):
    """Extract metadata about the repository"""
    data = {}
    
    # Languages
    lang_stats = soup.find_all(class_='Progress-item')
    if lang_stats:
        data['languages'] = {}
        for lang in lang_stats:
            name = lang.get('aria-label', '').split()[0]
            percentage = lang.get('style', '').replace('width:', '').replace('%;', '')
            if name and percentage:
                data['languages'][name] = float(percentage)

    # Stats & metadata
    stats = {}
    for link in soup.find_all('a', class_='Link--muted'):
        text = link.text.strip()
        for metric in ['star', 'fork', 'watch']:
            if metric in text.lower():
                stats[metric] = text.split()[0]
    if stats:
        data['stats'] = stats

    # Topics
    topics = soup.find_all('a', {'data-ga-click': 'Topic, repository page'})
    if topics:
        data['topics'] = [topic.text.strip() for topic in topics]

    # Last update
    last_update = soup.find('relative-time')
    if last_update:
        data['last_updated'] = last_update.get('datetime')

    return data

def extract_readme_content(soup):
    """Extract README content from repository page"""
    content = []
    
    readme = soup.find('article', class_='markdown-body') or soup.find('div', class_='Box-body')
    
    if readme:
        print(f"{Colors.OKBLUE}üìñ Extracting README content...{Colors.ENDC}")
        
        for unwanted in readme.find_all(['nav', 'footer', 'script', 'style']):
            unwanted.decompose()
        
        for element in readme.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'ul', 'ol']):
            if element.parent.name not in ['nav', 'footer']:
                if element.name.startswith('h'):
                    header_text = element.get_text(strip=True)
                    if header_text:
                        content.append(f"{'#' * int(element.name[1])} {header_text}")
                elif element.name == 'p':
                    text = element.get_text(strip=True)
                    if text:
                        content.append(text)
                elif element.name in ['ul', 'ol']:
                    for li in element.find_all('li', recursive=False):
                        li_text = li.get_text(strip=True)
                        if li_text:
                            content.append(f"- {li_text}")
        
        print(f"{Colors.OKGREEN}‚ú® Successfully extracted {len(content)} content blocks{Colors.ENDC}")
    
    return content

def extract_directory_content(soup):
    """Extract directory listing content"""
    content = []
    
    file_tree = soup.find('[role="grid"][aria-labelledby="files"]')
    if file_tree:
        content.append("# Directory Contents\n")
        for row in file_tree.find_all('[role="row"]'):
            link = row.find('a')
            if link:
                name = link.text.strip()
                path = link.get('href', '').strip()
                item_type = "üìÅ " if row.find('.octicon-file-directory') else "üìÑ "
                content.append(f"{item_type} {name}")
    
    return '\n'.join(content)

def extract_file_content(soup):
    """Extract file content"""
    code_element = soup.find('table', class_='highlight')
    if code_element:
        return code_element.get_text('\n', strip=True)
    return ""

def scrape_content(url):
    """Main function for scraping content from URLs"""
    print(f"\n{Colors.HEADER}üîç Starting to scrape: {url}{Colors.ENDC}")

    try:
        print(f"{Colors.OKBLUE}üì° Fetching page content...{Colors.ENDC}")
        response = requests.get(url)
        response.raise_for_status()

        # Handle different content types
        if 'github.com' in url:
            return handle_github_content(url, response)
        
        # Handle raw markdown
        if url.endswith('.md'):
            print(f"{Colors.OKBLUE}üìù Processing markdown content...{Colors.ENDC}")
            content = []
            
            lines = response.text.split('\n')
            for line in lines:
                line = line.strip()
                if line:
                    if line.startswith('#'):
                        content.append(line)
                    elif line.startswith('- ') or line.startswith('* '):
                        content.append(line)
                    else:
                        content.append(line)
            
            return {
                "title": "README.md",
                "text": "\n\n".join(content),
                "metadata": {
                    "url": url,
                    "crawled_at": datetime.now().isoformat()
                }
            }
        
        # Handle general content
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('h1').text.strip() if soup.find('h1') else "No Title Found"
        content = []
        article = soup.find('article') or soup.find(class_='entry-content')
        
        if article:
            content = extract_content_from_element(article)
        
        result = {
            "title": title,
            "text": "\n\n".join(content),
            "metadata": {
                "url": url,
                "crawled_at": datetime.now().isoformat()
            }
        }
        
        print(f"{Colors.OKGREEN}‚ú® Successfully scraped content:{Colors.ENDC}")
        print(f"    üìë Title: {title}")
        print(f"    üìä Content length: {len(result['text'])} characters")
        return result

    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Error scraping content: {str(e)}{Colors.ENDC}")
        return None

def handle_github_content(url, response):
    """Handle content scraping specifically for GitHub repositories"""
    soup = BeautifulSoup(response.text, 'html.parser')
    content = {
        'type': 'github',
        'title': '',
        'text': '',
        'metadata': {
            'url': url,
            'crawled_at': datetime.now().isoformat()
        }
    }

    # Parse repository structure
    path_parts = url.replace('https://github.com/', '').split('/')
    if len(path_parts) >= 2:
        content['metadata'].update({
            'owner': path_parts[0],
            'repo': path_parts[1],
            'path_type': path_parts[2] if len(path_parts) > 2 else 'root'
        })

    # Get title
    title_elem = soup.find('strong', {'itemprop': 'name'}) or soup.find('h1', class_='d-flex')
    content['title'] = title_elem.text.strip() if title_elem else path_parts[1]
    print(f"{Colors.OKBLUE}üìö Found repository: {content['title']}{Colors.ENDC}")

    # Extract metadata
    repo_data = extract_repository_data(soup)
    content['metadata'].update(repo_data)
    
    # Get description
    description = soup.find('p', {'class': 'f4'})
    if description:
        desc_text = description.text.strip()
        content['metadata']['description'] = desc_text
        print(f"{Colors.OKBLUE}üìã Description: {desc_text}{Colors.ENDC}")

    # Extract content based on path type
    if content['metadata']['path_type'] == 'root':
        readme_content = extract_readme_content(soup)
        if readme_content:
            content['text'] = '\n\n'.join(readme_content)
    elif content['metadata']['path_type'] == 'tree':
        content['text'] = extract_directory_content(soup)
    elif content['metadata']['path_type'] == 'blob':
        content['text'] = extract_file_content(soup)
    
    return content

def extract_content_from_element(element):
    """Extract formatted content from HTML elements"""
    content = []
    for child in element.children:
        if child.name == 'p':
            content.append(child.get_text(strip=True))
        elif child.name in ['h2', 'h3', 'h4', 'h5', 'h6']:
            content.append(f"{'#' * int(child.name[1])} {child.get_text(strip=True)}")
        elif child.name in ['ul', 'ol']:
            for li in child.find_all('li'):
                content.append(f"- {li.get_text(strip=True)}")
        elif child.string:
            content.append(child.string.strip())
    return content

def summarize_with_mistral(content):
    """Generate content summary using Mistral AI"""
    print(f"\n{Colors.HEADER}ü§ñ Starting Mistral AI summarization...{Colors.ENDC}")
    try:
        print(f"{Colors.OKBLUE}üìä Content length: {len(content)} characters{Colors.ENDC}")
        
        if len(content) > 20000:
            print(f"{Colors.OKBLUE}üìé Splitting large content into chunks...{Colors.ENDC}")
            chunks = split_content(content)
            print(f"{Colors.DIM}üîÑ Processing {len(chunks)} chunks{Colors.ENDC}")
            
            summaries = []
            for i, chunk in enumerate(chunks, 1):
                print(f"\n{Colors.OKBLUE}‚è≥ Processing chunk {i}/{len(chunks)}{Colors.ENDC}")
                try:
                    response = client.chat.complete(
                        model="mistral-large-latest",
                        messages=[{"role": "user", "content": f"Please summarize:\n\n{chunk}"}]
                    )
                    chunk_summary = response.choices[0].message.content
                    summaries.append(chunk_summary)
                    print(f"{Colors.OKGREEN}‚úì Chunk {i} complete{Colors.ENDC}")
                except Exception as e:
                    print(f"{Colors.FAIL}‚ùå Error on chunk {i}: {str(e)}{Colors.ENDC}")
                    summaries.append(f"[Summary failed for chunk {i}]")
            
            if len(summaries) > 1:
                print(f"\n{Colors.OKBLUE}üîÑ Combining summaries...{Colors.ENDC}")
                final_prompt = "Please provide a concise overall summary:\n\n"
                final_prompt += "\n\n".join([f"Part {i+1}:\n{s}" for i, s in enumerate(summaries)])
                
                response = client.chat.complete(
                    model="mistral-large-latest",
                    messages=[{"role": "user", "content": final_prompt}]
                )
                summary = response.choices[0].message.content
            else:
                summary = summaries[0]
        else:
            response = client.chat.complete(
                model="mistral-large-latest",
                messages=[{"role": "user", "content": f"Please summarize:\n\n{content}"}]
            )
            summary = response.choices[0].message.content
        
        print(f"{Colors.OKGREEN}‚ú® Summary generated successfully!{Colors.ENDC}")
        return summary

    except Exception as e:
        error_msg = str(e)
        print(f"{Colors.FAIL}‚ùå Summarization error: {error_msg}{Colors.ENDC}")
        
        if "api_key" in error_msg.lower():
            return "‚ùå Invalid API key. Check your MISTRAL_API_KEY in .env file."
        elif "timeout" in error_msg.lower():
            return "‚ùå Request timed out. Please try again."
        elif "too many requests" in error_msg.lower():
            return "‚ùå Rate limit exceeded. Please wait and try again."
        else:
            return f"‚ùå Error: {error_msg}"

def split_content(content, max_chars=20000):
    """Split content into chunks"""
    chunks = []
    current_chunk = []
    current_length = 0
    
    for line in content.split('\n'):
        line_length = len(line)
        if current_length + line_length + 1 <= max_chars:
            current_chunk.append(line)
            current_length += line_length + 1
        else:
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
            current_chunk = [line]
            current_length = line_length
    
    if current_chunk:
        chunks.append('\n'.join(current_chunk))
    
    return chunks

def save_content_json(content, output_file):
    """Save content to JSON file"""
    print(f"\n{Colors.OKBLUE}üíæ Saving JSON: {output_file}{Colors.ENDC}")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(content, f, indent=2, ensure_ascii=False)
    print(f"{Colors.OKGREEN}‚úì JSON saved{Colors.ENDC}")

def save_content_markdown(content, output_file):
    """Save content to Markdown file"""
    print(f"\n{Colors.OKBLUE}üìù Saving Markdown: {output_file}{Colors.ENDC}")
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"# {content['title']}\n\n")
        f.write(f"URL: {content['metadata']['url']}\n\n")
        f.write(f"Crawled at: {content['metadata']['crawled_at']}\n\n")
        f.write(f"## Summary\n\n{content['summary']}\n\n")
        f.write(content['text'])
    print(f"{Colors.OKGREEN}‚úì Markdown saved{Colors.ENDC}")

def main():
    print_dark_ascii()
    print(f"{Colors.CYAN}üåô Dark Mode Web Scraping Tool{Colors.ENDC}")
    print_disclaimer()
    
    print(f"\n{Colors.OKBLUE}üìå Enter URL (or press Enter for default):{Colors.ENDC}")
    user_input = input(f"{Colors.DIM}‚û°Ô∏è {Colors.ENDC}").strip()
    
    url = user_input if user_input else "https://example.com"
    
    raw_url = f"{url}/raw/master/README.md"
    print(f"\n{Colors.CYAN}üîó URL: {url}{Colors.ENDC}")
    
    # Try different README paths
    content = scrape_content(raw_url)
    if not content or not content.get('text'):
        print(f"{Colors.DIM}‚Üª Trying main branch...{Colors.ENDC}")
        raw_url = f"{url}/raw/main/README.md"
        content = scrape_content(raw_url)
        
    if not content or not content.get('text'):
        print(f"{Colors.DIM}‚Üª Trying repository page...{Colors.ENDC}")
        content = scrape_content(url)

    if content:
        print(f"\n{Colors.OKBLUE}‚ö° Processing content...{Colors.ENDC}")
        summary = summarize_with_mistral(content['text'])
        content['summary'] = summary

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("scraped_output", exist_ok=True)
        
        json_file = f"scraped_output/scraped_{timestamp}.json"
        md_file = f"scraped_output/scraped_{timestamp}.md"
        
        save_content_json(content, json_file)
        save_content_markdown(content, md_file)
        
        print(f"\n{Colors.OKGREEN}üéâ Operation complete:{Colors.ENDC}")
        print(f"{Colors.DIM}    üìÑ JSON: {json_file}")
        print(f"    üìù Markdown: {md_file}{Colors.ENDC}")
    else:
        print(f"\n{Colors.FAIL}‚ùå Failed to crawl content{Colors.ENDC}")

if __name__ == "__main__":
    main()
