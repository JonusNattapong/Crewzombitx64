{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ASCII Art\n",
    "ascii_art = \"\"\"\n",
    "███████╗ ██████╗ ███╗   ███╗██████╗ ██╗████████╗██╗  ██╗ ██████╗ ██╗  ██╗\n",
    "╚══███╔╝██╔═══██╗████╗ ████║██╔══██╗██║╚══██╔══╝╚██╗██╔╝██╔════╝ ██║  ██║\n",
    "  ███╔╝ ██║   ██║██╔████╔██║██████╔╝██║   ██║    ╚███╔╝ ███████╗ ███████║\n",
    " ███╔╝  ██║   ██║██║╚██╔╝██║██╔══██╗██║   ██║    ██╔██╗ ██╔═══██╗╚════██║\n",
    "███████╗╚██████╔╝██║ ╚═╝ ██║██████╔╝██║   ██║   ██╔╝ ██╗╚██████╔╝     ██║\n",
    "╚══════╝ ╚═════╝ ╚═╝     ╚═╝╚═════╝ ╚═╝   ╚═╝   ╚═╝  ╚═╝ ╚═════╝      ╚═╝\n",
    "\"\"\"\n",
    "print('\\033[95m' + ascii_art + '\\033[0m')  # Print in purple color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 ZombitX64 Web Scraper with Local Hugging Face Models\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. 🔗 Connecting to Google Drive\n",
    "2. 📦 Setting up Hugging Face transformers\n",
    "3. 🌐 Web scraping functionality\n",
    "4. 🤖 Local model inference\n",
    "5. 💾 Saving results to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers beautifulsoup4 requests\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "import os\n",
    "output_dir = '/content/drive/MyDrive/ZombitX64_Output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"✅ Output directory created at: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqGeneration\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Local Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"  # You can change this to other models\n",
    "print(f\"🤖 Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"✅ Model loaded successfully on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_robots_txt(url):\n",
    "    try:\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        if not parsed_url.netloc:\n",
    "            parsed_url = urllib.parse.urlparse(\"http://\" + url)\n",
    "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "        response = requests.get(robots_url)\n",
    "        response.raise_for_status()\n",
    "        return \"Disallow: /\" not in response.text\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error checking robots.txt: {str(e)}\")\n",
    "        return True\n",
    "\n",
    "def scrape_content(url):\n",
    "    if not check_robots_txt(url):\n",
    "        print(\"❌ Crawling disallowed by robots.txt\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        print(f\"🌐 Fetching content from: {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract title and content\n",
    "        title = soup.find('h1').text.strip() if soup.find('h1') else \"No Title Found\"\n",
    "        content = []\n",
    "\n",
    "        # Find main content\n",
    "        article = soup.find('article') or soup.find(class_='entry-content')\n",
    "        if article:\n",
    "            for elem in article.find_all(['p', 'h2', 'h3', 'ul', 'ol']):\n",
    "                content.append(elem.get_text(strip=True))\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"text\": \"\\n\\n\".join(content),\n",
    "            \"metadata\": {\n",
    "                \"url\": url,\n",
    "                \"crawled_at\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping content: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_length=1024):\n",
    "    try:\n",
    "        print(\"🤖 Generating summary...\")\n",
    "        inputs = tokenizer(text, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150,\n",
    "            min_length=40,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        print(\"✅ Summary generated successfully!\")\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating summary: {str(e)}\")\n",
    "        return \"Summary generation failed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(content, summary):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save JSON\n",
    "    json_path = f\"{output_dir}/scraped_{timestamp}.json\"\n",
    "    content_with_summary = {\n",
    "        **content,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content_with_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save Markdown\n",
    "    md_path = f\"{output_dir}/scraped_{timestamp}.md\"\n",
    "    with open(md_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# {content['title']}\\n\\n\")\n",
    "        f.write(f\"URL: {content['metadata']['url']}\\n\\n\")\n",
    "        f.write(f\"Crawled at: {content['metadata']['crawled_at']}\\n\\n\")\n",
    "        f.write(f\"## Summary\\n\\n{summary}\\n\\n\")\n",
    "        f.write(content['text'])\n",
    "    \n",
    "    print(f\"✅ Results saved to Google Drive:\")\n",
    "    print(f\"📄 JSON: {json_path}\")\n",
    "    print(f\"📝 Markdown: {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input URL\n",
    "url = input(\"🌐 Enter the URL to scrape (or press Enter for default): \") or \"https://example.com\"\n",
    "\n",
    "# Scrape content\n",
    "content = scrape_content(url)\n",
    "\n",
    "if content:\n",
    "    # Generate summary\n",
    "    summary = summarize_text(content['text'])\n",
    "    \n",
    "    # Save results\n",
    "    save_results(content, summary)\n",
    "    \n",
    "    # Show ASCII art again for completion\n",
    "    print('\\n\\033[95m' + ascii_art + '\\033[0m')\n",
    "    print(\"✨ Task completed successfully! ✨\")\n",
    "else:\n",
    "    print(\"❌ Failed to scrape content\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ZombitX64_Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
